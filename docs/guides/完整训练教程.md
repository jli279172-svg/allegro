# Allegro åŠ¿å‡½æ•°è®­ç»ƒå®Œæ•´æ•™ç¨‹

## ğŸ“š ç›®å½•

1. [é¡¹ç›®ç®€ä»‹](#é¡¹ç›®ç®€ä»‹)
2. [ç¯å¢ƒå‡†å¤‡](#ç¯å¢ƒå‡†å¤‡)
3. [æ•°æ®å‡†å¤‡](#æ•°æ®å‡†å¤‡)
4. [é…ç½®æ–‡ä»¶è¯¦è§£](#é…ç½®æ–‡ä»¶è¯¦è§£)
5. [å¼€å§‹è®­ç»ƒ](#å¼€å§‹è®­ç»ƒ)
6. [ç›‘æ§è®­ç»ƒè¿‡ç¨‹](#ç›‘æ§è®­ç»ƒè¿‡ç¨‹)
7. [è¯„ä¼°æ¨¡å‹æ€§èƒ½](#è¯„ä¼°æ¨¡å‹æ€§èƒ½)
8. [ä½¿ç”¨è®­ç»ƒå¥½çš„æ¨¡å‹](#ä½¿ç”¨è®­ç»ƒå¥½çš„æ¨¡å‹)
9. [å¸¸è§é—®é¢˜ä¸è§£å†³æ–¹æ¡ˆ](#å¸¸è§é—®é¢˜ä¸è§£å†³æ–¹æ¡ˆ)
10. [è¿›é˜¶æŠ€å·§](#è¿›é˜¶æŠ€å·§)

---

## é¡¹ç›®ç®€ä»‹

Allegro æ˜¯ä¸€ä¸ªåŸºäº E(3)-ç­‰å˜ç¥ç»ç½‘ç»œçš„é«˜ç²¾åº¦æœºå™¨å­¦ä¹ åŠ¿å‡½æ•°æ¨¡å‹ï¼Œç”¨äºåŸå­å°ºåº¦æ¨¡æ‹Ÿã€‚æœ¬æ•™ç¨‹å°†æŒ‡å¯¼ä½ å¦‚ä½•ä½¿ç”¨è¿™ä¸ªç‰ˆæœ¬çš„ Allegro è®­ç»ƒè‡ªå·±çš„åŠ¿å‡½æ•°ã€‚

### é¡¹ç›®ç»“æ„

```
allegro/
â”œâ”€â”€ allegro/              # æ ¸å¿ƒä»£ç åŒ…
â”œâ”€â”€ configs/              # è®­ç»ƒé…ç½®æ–‡ä»¶
â”‚   â”œâ”€â”€ dataset_1593_mps.yaml    # MPS (Apple Silicon) é…ç½®
â”‚   â”œâ”€â”€ dataset_1593_gpu.yaml    # GPU é…ç½®
â”‚   â””â”€â”€ tutorial.yaml            # æ•™ç¨‹é…ç½®
â”œâ”€â”€ data/                 # è®­ç»ƒæ•°æ®
â”‚   â””â”€â”€ dataset_1593.xyz  # ç¤ºä¾‹æ•°æ®é›†
â”œâ”€â”€ scripts/              # è¾…åŠ©è„šæœ¬
â”‚   â”œâ”€â”€ start_training.sh  # è®­ç»ƒå¯åŠ¨è„šæœ¬
â”‚   â””â”€â”€ ...
â”œâ”€â”€ experiments/           # å®éªŒè¾“å‡º
â”‚   â””â”€â”€ outputs/          # è®­ç»ƒè¾“å‡ºç›®å½•
â”œâ”€â”€ logs/                 # æ—¥å¿—æ–‡ä»¶
â””â”€â”€ docs/                 # æ–‡æ¡£
```

---

## ç¯å¢ƒå‡†å¤‡

### 1. Python ç¯å¢ƒ

**è¦æ±‚ï¼š**
- Python >= 3.10
- æ¨èä½¿ç”¨è™šæ‹Ÿç¯å¢ƒï¼ˆvenv æˆ– condaï¼‰

**åˆ›å»ºè™šæ‹Ÿç¯å¢ƒï¼š**

```bash
# ä½¿ç”¨ venv
python3 -m venv .venv
source .venv/bin/activate  # macOS/Linux
# æˆ–
.venv\Scripts\activate  # Windows

# ä½¿ç”¨ conda
conda create -n allegro python=3.10
conda activate allegro
```

### 2. å®‰è£…ä¾èµ–

#### 2.1 å®‰è£… PyTorch

**æ ¹æ®ä½ çš„ç¡¬ä»¶å¹³å°é€‰æ‹©ï¼š**

**Apple Silicon (M1/M2/M3/M4) - ä½¿ç”¨ MPSï¼š**
```bash
pip install torch torchvision torchaudio
```

**NVIDIA GPU - ä½¿ç”¨ CUDAï¼š**
```bash
# CUDA 11.8
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118

# CUDA 12.1
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121
```

**CPU ç‰ˆæœ¬ï¼š**
```bash
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu
```

#### 2.2 å®‰è£… NequIP å’Œ Allegro

```bash
# å®‰è£… nequip-allegroï¼ˆä¼šè‡ªåŠ¨å®‰è£… nequipï¼‰
pip install nequip-allegro

# æˆ–è€…ä»æºç å®‰è£…ï¼ˆå¼€å‘æ¨¡å¼ï¼‰
cd /Users/lijunchen/coding/allegro
pip install -e .
```

#### 2.3 éªŒè¯å®‰è£…

```bash
# æ£€æŸ¥ PyTorch ç‰ˆæœ¬
python -c "import torch; print(f'PyTorch: {torch.__version__}')"

# æ£€æŸ¥ MPS æ”¯æŒï¼ˆApple Siliconï¼‰
python -c "import torch; print(f'MPS available: {torch.backends.mps.is_available()}')"

# æ£€æŸ¥ CUDA æ”¯æŒï¼ˆNVIDIA GPUï¼‰
python -c "import torch; print(f'CUDA available: {torch.cuda.is_available()}')"

# æ£€æŸ¥ nequip å’Œ allegro
python -c "import nequip; import allegro; print('Installation successful!')"
```

### 3. æ£€æŸ¥è®­ç»ƒå‘½ä»¤

```bash
# æ£€æŸ¥ nequip-train å‘½ä»¤æ˜¯å¦å¯ç”¨
which nequip-train
nequip-train --help
```

---

## æ•°æ®å‡†å¤‡

### 1. æ•°æ®æ ¼å¼è¦æ±‚

Allegro ä½¿ç”¨ **extxyz** æ ¼å¼çš„è®­ç»ƒæ•°æ®ã€‚æ¯ä¸ªç»“æ„åŒ…å«ï¼š

- **åŸå­åæ ‡** (positions)
- **åŸå­ç±»å‹** (species/elements)
- **æ€»èƒ½é‡** (total energy)
- **åŸå­å—åŠ›** (forces) - æ¯ä¸ªåŸå­çš„ 3D åŠ›å‘é‡
- **å¯é€‰ï¼š** æ™¶æ ¼å‚æ•° (lattice)ã€å‘¨æœŸæ€§è¾¹ç•Œæ¡ä»¶ (pbc)

### 2. æ•°æ®æ ¼å¼ç¤ºä¾‹

```extxyz
192
TotEnergy=-1103.24236000 cutoff=-1.00000000 nneightol=1.20000000 pbc="T T T" Lattice="23.46511000 0.00000000 0.00000000 -0.00000100 23.46511000 0.00000000 -0.00000100 -0.00000100 23.46511000" Properties=species:S:1:pos:R:3:force:R:3:Z:I:1
O  11.72590000  14.59020000  25.33440000  -0.04213780  0.03788820  0.00314949  8
H  12.69400000  16.13880000  24.72010000  -0.03709700  -0.03453660  0.01566490  1
H   9.70021000  15.03790000  25.76530000   0.07676920  -0.00101183  -0.02270490  1
...
```

**æ ¼å¼è¯´æ˜ï¼š**
- ç¬¬ä¸€è¡Œï¼šåŸå­æ•°é‡
- ç¬¬äºŒè¡Œï¼šå…ƒæ•°æ®ï¼ˆèƒ½é‡ã€æ™¶æ ¼ã€å±æ€§å®šä¹‰ï¼‰
- åç»­è¡Œï¼šæ¯ä¸ªåŸå­çš„ä¿¡æ¯ï¼ˆå…ƒç´ ã€åæ ‡ã€åŠ›ã€åŸå­åºæ•°ï¼‰

### 3. æ•°æ®å­—æ®µæ˜ å°„

åœ¨é…ç½®æ–‡ä»¶ä¸­ï¼Œéœ€è¦å°†ä½ çš„æ•°æ®å­—æ®µæ˜ å°„åˆ° NequIP çš„æ ‡å‡†å­—æ®µï¼š

```yaml
key_mapping:
  TotEnergy: total_energy  # ä½ çš„æ•°æ®ä¸­çš„æ€»èƒ½é‡å­—æ®µå
  force: forces            # ä½ çš„æ•°æ®ä¸­çš„åŠ›å­—æ®µåï¼ˆå•æ•°â†’å¤æ•°ï¼‰
```

### 4. å‡†å¤‡ä½ çš„æ•°æ®

**å¦‚æœä½ çš„æ•°æ®æ˜¯å…¶ä»–æ ¼å¼ï¼Œéœ€è¦è½¬æ¢ï¼š**

```python
# ç¤ºä¾‹ï¼šä» ASE Atoms å¯¹è±¡è½¬æ¢ä¸º extxyz
from ase import Atoms
from ase.io import write

# åˆ›å»ºæˆ–åŠ è½½ä½ çš„ç»“æ„
atoms = Atoms(...)  # ä½ çš„åŸå­ç»“æ„
atoms.info['TotEnergy'] = energy  # è®¾ç½®æ€»èƒ½é‡
atoms.arrays['force'] = forces    # è®¾ç½®åŠ›ï¼ˆæ³¨æ„å­—æ®µåï¼‰

# ä¿å­˜ä¸º extxyz
write('your_data.xyz', atoms, format='extxyz')
```

### 5. æ•°æ®é›†åˆ’åˆ†

å»ºè®®çš„æ•°æ®é›†åˆ’åˆ†æ¯”ä¾‹ï¼š
- **è®­ç»ƒé›†**: 70-80%
- **éªŒè¯é›†**: 10-15%
- **æµ‹è¯•é›†**: 10-15%

åœ¨é…ç½®æ–‡ä»¶ä¸­æŒ‡å®šï¼š
```yaml
split_dataset:
  - file_path: data/your_data.xyz
    train: 1200  # è®­ç»ƒé›†ç»“æ„æ•°
    val: 200     # éªŒè¯é›†ç»“æ„æ•°
    test: 193    # æµ‹è¯•é›†ç»“æ„æ•°
```

---

## é…ç½®æ–‡ä»¶è¯¦è§£

### 1. é…ç½®æ–‡ä»¶ä½ç½®

é…ç½®æ–‡ä»¶ä½äº `configs/` ç›®å½•ï¼š
- `dataset_1593_mps.yaml` - Apple Silicon (MPS) é…ç½®
- `dataset_1593_gpu.yaml` - NVIDIA GPU é…ç½®
- `tutorial.yaml` - åŸºç¡€æ•™ç¨‹é…ç½®

### 2. é…ç½®æ–‡ä»¶ç»“æ„

#### 2.1 åŸºæœ¬è®¾ç½®

```yaml
# è¿è¡Œæ¨¡å¼ï¼štrainï¼ˆè®­ç»ƒï¼‰ã€testï¼ˆæµ‹è¯•ï¼‰ã€train+testï¼ˆè®­ç»ƒ+æµ‹è¯•ï¼‰
run: [train, test]

# æˆªæ–­åŠå¾„ï¼ˆÃ…ï¼‰- å†³å®šæ¨¡å‹è€ƒè™‘å¤šè¿œçš„é‚»å±…åŸå­
cutoff_radius: 5.0

# åŸå­ç±»å‹åˆ—è¡¨
model_type_names: [H, O]  # æ ¹æ®ä½ çš„ç³»ç»Ÿä¿®æ”¹
chemical_species: ${model_type_names}
```

#### 2.2 æ•°æ®é…ç½®

```yaml
data:
  _target_: nequip.data.datamodule.ASEDataModule
  seed: 123  # éšæœºç§å­ï¼Œç¡®ä¿å¯é‡å¤æ€§
  
  # æ•°æ®é›†åˆ’åˆ†
  split_dataset:
    - file_path: /path/to/your/data.xyz  # æ•°æ®æ–‡ä»¶è·¯å¾„
      train: 1200  # è®­ç»ƒé›†å¤§å°
      val: 200     # éªŒè¯é›†å¤§å°
      test: 193    # æµ‹è¯•é›†å¤§å°
  
  # æ•°æ®è½¬æ¢
  transforms:
    - _target_: nequip.data.transforms.ChemicalSpeciesToAtomTypeMapper
      chemical_symbols: ${model_type_names}
    - _target_: nequip.data.transforms.NeighborListTransform
      r_max: ${cutoff_radius}
  
  # æ•°æ®æ ¼å¼
  ase_args:
    format: extxyz
  
  # å­—æ®µæ˜ å°„ï¼ˆé‡è¦ï¼ï¼‰
  key_mapping:
    TotEnergy: total_energy  # ä½ çš„èƒ½é‡å­—æ®µ â†’ NequIP æ ‡å‡†å­—æ®µ
    force: forces            # ä½ çš„åŠ›å­—æ®µ â†’ NequIP æ ‡å‡†å­—æ®µ
  
  # æ•°æ®åŠ è½½å™¨è®¾ç½®
  train_dataloader:
    _target_: torch.utils.data.DataLoader
    batch_size: 4      # æ‰¹æ¬¡å¤§å°ï¼ˆæ ¹æ®å†…å­˜è°ƒæ•´ï¼‰
    num_workers: 2     # æ•°æ®åŠ è½½çº¿ç¨‹æ•°
    pin_memory: false  # MPS ä¸éœ€è¦ï¼ŒGPU å¯ä»¥è®¾ä¸º true
  
  val_dataloader:
    _target_: torch.utils.data.DataLoader
    batch_size: 8      # éªŒè¯å¯ä»¥ç”¨æ›´å¤§çš„ batch
    num_workers: 2
    pin_memory: false
```

#### 2.3 è®­ç»ƒå™¨é…ç½®

```yaml
trainer:
  _target_: lightning.Trainer
  max_epochs: 100           # æœ€å¤§è®­ç»ƒè½®æ•°
  log_every_n_steps: 10     # æ¯ N æ­¥è®°å½•ä¸€æ¬¡æ—¥å¿—
  accelerator: cpu          # è®¾å¤‡ï¼šcpu / gpu / mps
  devices: 1                # è®¾å¤‡æ•°é‡
  enable_progress_bar: true # æ˜¾ç¤ºè¿›åº¦æ¡
  
  # æ¨¡å‹æ£€æŸ¥ç‚¹å›è°ƒ
  callbacks:
    - _target_: lightning.pytorch.callbacks.ModelCheckpoint
      dirpath: ${hydra:runtime.output_dir}  # è¾“å‡ºç›®å½•
      save_last: true                       # ä¿å­˜æœ€åä¸€ä¸ªæ¨¡å‹
      save_top_k: 1                         # ä¿å­˜æœ€ä½³æ¨¡å‹
      monitor: val0_epoch/weighted_sum      # ç›‘æ§æŒ‡æ ‡
      mode: min                            # æœ€å°åŒ–æŒ‡æ ‡
      filename: best                       # æœ€ä½³æ¨¡å‹æ–‡ä»¶å
```

#### 2.4 æ¨¡å‹é…ç½®

```yaml
# æ ¸å¿ƒè¶…å‚æ•°
num_scalar_features: 64  # æ ‡é‡ç‰¹å¾æ•°ï¼ˆ16, 32, 64, 128, 256ï¼‰

training_module:
  _target_: nequip.train.EMALightningModule
  
  # æŸå¤±å‡½æ•°
  loss:
    _target_: nequip.train.EnergyForceLoss
    per_atom_energy: true
    coeffs:
      total_energy: 1.0  # èƒ½é‡æŸå¤±æƒé‡
      forces: 1.0        # åŠ›æŸå¤±æƒé‡
  
  # ä¼˜åŒ–å™¨
  optimizer:
    _target_: torch.optim.Adam
    lr: 0.001  # å­¦ä¹ ç‡ï¼ˆAllegro æ¨è 1e-3ï¼‰
  
  # æ¨¡å‹å®šä¹‰
  model:
    _target_: allegro.model.AllegroModel
    
    # åŸºæœ¬å‚æ•°
    seed: 456
    model_dtype: float32
    type_names: ${model_type_names}
    r_max: ${cutoff_radius}
    
    # å¾„å‘åµŒå…¥
    radial_chemical_embed:
      _target_: allegro.nn.TwoBodyBesselScalarEmbed
      num_bessels: 8
      bessel_trainable: false
      polynomial_cutoff_p: 6
    
    radial_chemical_embed_dim: ${num_scalar_features}
    
    # æ ¸å¿ƒè¶…å‚æ•°ï¼ˆæœ€é‡è¦çš„è°ƒå‚å¯¹è±¡ï¼‰
    l_max: 1              # çƒè°å‡½æ•°æœ€å¤§é˜¶æ•°ï¼ˆ1=å¿«ï¼Œ2=å‡†ï¼Œ3=å¾ˆå‡†ä½†æ…¢ï¼‰
    num_layers: 2         # å±‚æ•°ï¼ˆ1-3 é€šå¸¸æœ€ä½³ï¼‰
    num_scalar_features: ${num_scalar_features}
    num_tensor_features: 32  # å¼ é‡ç‰¹å¾æ•°ï¼ˆ8, 16, 32, 64ï¼‰
    
    # MLP è®¾ç½®
    scalar_embed_mlp_hidden_layers_depth: 1
    scalar_embed_mlp_hidden_layers_width: ${num_scalar_features}
    scalar_embed_mlp_nonlinearity: silu
    
    allegro_mlp_hidden_layers_depth: 1
    allegro_mlp_hidden_layers_width: ${num_scalar_features}
    allegro_mlp_nonlinearity: silu
    
    readout_mlp_hidden_layers_depth: 1
    readout_mlp_hidden_layers_width: ${num_scalar_features}
    readout_mlp_nonlinearity: silu
    
    # é«˜çº§å‚æ•°ï¼ˆé€šå¸¸ä¿æŒé»˜è®¤ï¼‰
    parity: true
    tp_path_channel_coupling: true
    
    # ç»Ÿè®¡ä¿¡æ¯ï¼ˆè‡ªåŠ¨ä»æ•°æ®è®¡ç®—ï¼‰
    avg_num_neighbors: ${training_data_stats:num_neighbors_mean}
    per_type_energy_shifts: ${training_data_stats:per_atom_energy_mean}
    per_type_energy_scales: ${training_data_stats:forces_rms}
```

### 3. å¹³å°ç‰¹å®šé…ç½®

#### 3.1 Apple Silicon (MPS)

```yaml
trainer:
  accelerator: cpu  # MPS å¯èƒ½æœ‰å…¼å®¹æ€§é—®é¢˜ï¼Œå»ºè®®å…ˆç”¨ CPU
  # æˆ–å°è¯• accelerator: mpsï¼ˆå¦‚æœæ”¯æŒï¼‰

train_dataloader:
  batch_size: 4      # è¾ƒå°çš„ batch size
  num_workers: 2     # å‡å°‘ workers
  pin_memory: false  # MPS ä¸éœ€è¦

model:
  compile_mode: null  # MPS ä¸Š torch.compile å¯èƒ½ä¸ç¨³å®š
```

#### 3.2 NVIDIA GPU

```yaml
trainer:
  accelerator: gpu
  devices: 1

train_dataloader:
  batch_size: 8      # å¯ä»¥æ›´å¤§
  num_workers: 4
  pin_memory: true   # GPU åŠ é€Ÿæ•°æ®ä¼ è¾“

model:
  compile_mode: compile  # PyTorch >= 2.6.0 å¯ä»¥å¯ç”¨
```

#### 3.3 CPU

```yaml
trainer:
  accelerator: cpu
  devices: 1

train_dataloader:
  batch_size: 2      # è¾ƒå°çš„ batch
  num_workers: 4     # å¯ä»¥æ›´å¤š workers
  pin_memory: false
```

---

## å¼€å§‹è®­ç»ƒ

### æ–¹æ³• 1: ä½¿ç”¨å‘½ä»¤è¡Œï¼ˆæ¨èï¼‰

#### åŸºæœ¬å‘½ä»¤

```bash
# è¿›å…¥é¡¹ç›®ç›®å½•
cd /Users/lijunchen/coding/allegro

# ä½¿ç”¨é…ç½®æ–‡ä»¶è®­ç»ƒ
nequip-train configs/dataset_1593_mps.yaml
```

#### ä½¿ç”¨ Hydra é…ç½®ç³»ç»Ÿ

```bash
# ä½¿ç”¨ config-path å’Œ config-name
nequip-train --config-path=configs --config-name=dataset_1593_mps

# è¦†ç›–é…ç½®å‚æ•°
nequip-train configs/dataset_1593_mps.yaml \
  trainer.max_epochs=50 \
  data.train_dataloader.batch_size=8
```

### æ–¹æ³• 2: ä½¿ç”¨è®­ç»ƒè„šæœ¬

```bash
# ä½¿ç”¨æä¾›çš„è®­ç»ƒè„šæœ¬
bash scripts/start_training.sh

# æˆ–ç›´æ¥æ‰§è¡Œ
./scripts/start_training.sh
```

### æ–¹æ³• 3: åœ¨åå°è¿è¡Œ

```bash
# ä½¿ç”¨ nohup
nohup nequip-train configs/dataset_1593_mps.yaml > training.log 2>&1 &

# ä½¿ç”¨ screen
screen -S allegro_training
nequip-train configs/dataset_1593_mps.yaml
# æŒ‰ Ctrl+A ç„¶å D åˆ†ç¦»ä¼šè¯
# é‡æ–°è¿æ¥ï¼šscreen -r allegro_training

# ä½¿ç”¨ tmux
tmux new -s allegro_training
nequip-train configs/dataset_1593_mps.yaml
# æŒ‰ Ctrl+B ç„¶å D åˆ†ç¦»ä¼šè¯
# é‡æ–°è¿æ¥ï¼štmux attach -t allegro_training
```

### è®­ç»ƒè¾“å‡º

è®­ç»ƒå¼€å§‹åï¼Œä¼šåˆ›å»ºè¾“å‡ºç›®å½•ï¼ˆé€šå¸¸åœ¨ `experiments/outputs/YYYY-MM-DD/HH-MM-SS/`ï¼‰ï¼ŒåŒ…å«ï¼š

```
experiments/outputs/2025-01-19/14-30-00/
â”œâ”€â”€ best.ckpt              # æœ€ä½³æ¨¡å‹ï¼ˆéªŒè¯æŸå¤±æœ€å°ï¼‰
â”œâ”€â”€ last.ckpt              # æœ€åä¸€ä¸ª epoch çš„æ¨¡å‹
â”œâ”€â”€ config.yaml            # ä½¿ç”¨çš„é…ç½®æ–‡ä»¶
â”œâ”€â”€ metrics.csv            # è®­ç»ƒæŒ‡æ ‡ CSV
â””â”€â”€ train.log              # è®­ç»ƒæ—¥å¿—
```

---

## ç›‘æ§è®­ç»ƒè¿‡ç¨‹

### 1. å®æ—¶ç›‘æ§

è®­ç»ƒè¿‡ç¨‹ä¸­ä¼šæ˜¾ç¤ºï¼š
- å½“å‰ epoch
- è®­ç»ƒ/éªŒè¯æŸå¤±
- èƒ½é‡å’ŒåŠ›çš„ MAEï¼ˆå¹³å‡ç»å¯¹è¯¯å·®ï¼‰
- è¿›åº¦æ¡

### 2. æŸ¥çœ‹æ—¥å¿—æ–‡ä»¶

```bash
# æŸ¥çœ‹æœ€æ–°çš„è®­ç»ƒæ—¥å¿—
tail -f experiments/outputs/æœ€æ–°ç›®å½•/train.log

# æŸ¥çœ‹æŒ‡æ ‡ CSV
cat experiments/outputs/æœ€æ–°ç›®å½•/metrics.csv
```

### 3. ä½¿ç”¨ TensorBoardï¼ˆå¦‚æœå¯ç”¨ï¼‰

```bash
# å¯åŠ¨ TensorBoard
tensorboard --logdir=experiments/outputs

# åœ¨æµè§ˆå™¨æ‰“å¼€ http://localhost:6006
```

### 4. æ£€æŸ¥è®­ç»ƒæŒ‡æ ‡

å…³é”®æŒ‡æ ‡ï¼š
- **weighted_sum**: åŠ æƒæ€»æŸå¤±ï¼ˆè¶Šå°è¶Šå¥½ï¼‰
- **total_energy_mae**: æ€»èƒ½é‡ MAEï¼ˆeVï¼‰
- **forces_mae**: åŠ› MAEï¼ˆeV/Ã…ï¼‰
- **per_atom_energy_mae**: æ¯åŸå­èƒ½é‡ MAEï¼ˆeV/atomï¼‰

---

## è¯„ä¼°æ¨¡å‹æ€§èƒ½

### 1. è‡ªåŠ¨æµ‹è¯•ï¼ˆå¦‚æœé…ç½®äº† testï¼‰

å¦‚æœé…ç½®æ–‡ä»¶ä¸­åŒ…å« `run: [train, test]`ï¼Œè®­ç»ƒå®Œæˆåä¼šè‡ªåŠ¨åœ¨æµ‹è¯•é›†ä¸Šè¯„ä¼°ã€‚

### 2. æ‰‹åŠ¨è¯„ä¼°

```bash
# ä½¿ç”¨è®­ç»ƒå¥½çš„æ¨¡å‹è¯„ä¼°
nequip-evaluate \
  --model-dir experiments/outputs/2025-01-19/14-30-00 \
  --config experiments/outputs/2025-01-19/14-30-00/config.yaml
```

### 3. æŸ¥çœ‹è¯„ä¼°æŠ¥å‘Š

è®­ç»ƒå®Œæˆåï¼ŒæŸ¥çœ‹ `reports/æµ‹è¯•è¯„ä¼°æŠ¥å‘Š.md` äº†è§£è¯„ä¼°æŒ‡æ ‡çš„å«ä¹‰ã€‚

---

## ä½¿ç”¨è®­ç»ƒå¥½çš„æ¨¡å‹

### 1. æ¨¡å‹æ–‡ä»¶ä½ç½®

è®­ç»ƒå®Œæˆåï¼Œæ¨¡å‹æ–‡ä»¶ä½äºï¼š
```
experiments/outputs/YYYY-MM-DD/HH-MM-SS/
â”œâ”€â”€ best.ckpt    # æœ€ä½³æ¨¡å‹ï¼ˆæ¨èä½¿ç”¨ï¼‰
â””â”€â”€ last.ckpt    # æœ€åä¸€ä¸ª epoch çš„æ¨¡å‹
```

### 2. åœ¨ Python ä¸­ä½¿ç”¨

```python
from nequip.utils import load_model
import torch

# åŠ è½½æ¨¡å‹
model, metadata = load_model(
    model_path="experiments/outputs/2025-01-19/14-30-00/best.ckpt",
    device="cpu"  # æˆ– "cuda" / "mps"
)

# å‡†å¤‡è¾“å…¥æ•°æ®ï¼ˆASE Atoms å¯¹è±¡ï¼‰
from ase import Atoms
atoms = Atoms(...)  # ä½ çš„åŸå­ç»“æ„

# é¢„æµ‹
energy = model(atoms)
forces = model.get_forces(atoms)
```

### 3. å¯¼å‡ºä¸º LAMMPS æ ¼å¼

```bash
# ä½¿ç”¨ç¼–è¯‘è„šæœ¬
bash scripts/compile_for_lammps.sh \
  experiments/outputs/2025-01-19/14-30-00/best.ckpt \
  experiments/outputs/2025-01-19/14-30-00/lammps_model.pt2

# æˆ–ä½¿ç”¨ nequip-compile å‘½ä»¤
nequip-compile \
  experiments/outputs/2025-01-19/14-30-00/best.ckpt \
  experiments/outputs/2025-01-19/14-30-00/lammps_model.pt2 \
  --device cpu \
  --mode aotinductor \
  --target pair_allegro
```

### 4. åœ¨ LAMMPS ä¸­ä½¿ç”¨

```lammps
# LAMMPS è¾“å…¥æ–‡ä»¶ç¤ºä¾‹
pair_style nequip
pair_coeff * * experiments/outputs/2025-01-19/14-30-00/lammps_model.pt2 H O
```

---

## å¸¸è§é—®é¢˜ä¸è§£å†³æ–¹æ¡ˆ

### é—®é¢˜ 1: æ‰¾ä¸åˆ°æ•°æ®æ–‡ä»¶

**é”™è¯¯ä¿¡æ¯ï¼š**
```
FileNotFoundError: [Errno 2] No such file or directory: 'data/dataset_1593.xyz'
```

**è§£å†³æ–¹æ¡ˆï¼š**
1. æ£€æŸ¥æ•°æ®æ–‡ä»¶è·¯å¾„æ˜¯å¦æ­£ç¡®
2. ä½¿ç”¨ç»å¯¹è·¯å¾„æˆ–ç›¸å¯¹äºé¡¹ç›®æ ¹ç›®å½•çš„è·¯å¾„
3. ç¡®ä¿ä» `allegro/` ç›®å½•è¿è¡Œå‘½ä»¤

```yaml
# åœ¨é…ç½®æ–‡ä»¶ä¸­ä½¿ç”¨ç»å¯¹è·¯å¾„
file_path: /Users/lijunchen/coding/allegro/data/dataset_1593.xyz

# æˆ–ç›¸å¯¹è·¯å¾„ï¼ˆç›¸å¯¹äºè¿è¡Œå‘½ä»¤çš„ç›®å½•ï¼‰
file_path: data/dataset_1593.xyz
```

### é—®é¢˜ 2: MPS/CUDA ä¸å¯ç”¨

**æ£€æŸ¥æ–¹æ³•ï¼š**
```python
import torch
print(f"MPS available: {torch.backends.mps.is_available()}")
print(f"CUDA available: {torch.cuda.is_available()}")
```

**è§£å†³æ–¹æ¡ˆï¼š**
- å¦‚æœ MPS/CUDA ä¸å¯ç”¨ï¼Œä½¿ç”¨ CPUï¼š
```yaml
trainer:
  accelerator: cpu
```

### é—®é¢˜ 3: å†…å­˜ä¸è¶³ (OOM)

**é”™è¯¯ä¿¡æ¯ï¼š**
```
RuntimeError: CUDA out of memory
```

**è§£å†³æ–¹æ¡ˆï¼š**
1. å‡å° batch sizeï¼š
```yaml
train_dataloader:
  batch_size: 2  # ä» 4 å‡å°åˆ° 2
```

2. å‡å°æ¨¡å‹å¤§å°ï¼š
```yaml
num_scalar_features: 32  # ä» 64 å‡å°åˆ° 32
num_tensor_features: 16  # ä» 32 å‡å°åˆ° 16
```

3. å‡å°æˆªæ–­åŠå¾„ï¼š
```yaml
cutoff_radius: 4.0  # ä» 5.0 å‡å°åˆ° 4.0
```

### é—®é¢˜ 4: æ•°æ®å­—æ®µæ˜ å°„é”™è¯¯

**é”™è¯¯ä¿¡æ¯ï¼š**
```
KeyError: 'total_energy'
```

**è§£å†³æ–¹æ¡ˆï¼š**
æ£€æŸ¥é…ç½®æ–‡ä»¶ä¸­çš„ `key_mapping`ï¼Œç¡®ä¿å­—æ®µåæ­£ç¡®ï¼š
```yaml
key_mapping:
  TotEnergy: total_energy  # ä½ çš„æ•°æ®ä¸­çš„å­—æ®µå
  force: forces
```

### é—®é¢˜ 5: è®­ç»ƒæŸå¤±ä¸ä¸‹é™

**å¯èƒ½åŸå› å’Œè§£å†³æ–¹æ¡ˆï¼š**

1. **å­¦ä¹ ç‡å¤ªå¤§æˆ–å¤ªå°**
   - å°è¯•è°ƒæ•´å­¦ä¹ ç‡ï¼š`0.0001` åˆ° `0.01`
   - Allegro æ¨èï¼š`0.001`

2. **æ•°æ®è´¨é‡é—®é¢˜**
   - æ£€æŸ¥æ•°æ®æ˜¯å¦åŒ…å«å¼‚å¸¸å€¼
   - ç¡®ä¿èƒ½é‡å’ŒåŠ›çš„å•ä½ä¸€è‡´

3. **æ¨¡å‹å®¹é‡ä¸è¶³**
   - å¢åŠ  `num_scalar_features` æˆ– `num_layers`
   - å¢åŠ  `l_max`ï¼ˆä½†ä¼šå˜æ…¢ï¼‰

4. **æˆªæ–­åŠå¾„å¤ªå°**
   - å¢åŠ  `cutoff_radius`

### é—®é¢˜ 6: è®­ç»ƒä¸­æ–­åå¦‚ä½•æ¢å¤

**æ–¹æ³• 1: ä»æ£€æŸ¥ç‚¹æ¢å¤**
```yaml
# åœ¨é…ç½®æ–‡ä»¶ä¸­æ·»åŠ 
trainer:
  resume_from_checkpoint: experiments/outputs/YYYY-MM-DD/HH-MM-SS/last.ckpt
```

**æ–¹æ³• 2: ä½¿ç”¨å‘½ä»¤è¡Œå‚æ•°**
```bash
nequip-train configs/dataset_1593_mps.yaml \
  trainer.resume_from_checkpoint=experiments/outputs/YYYY-MM-DD/HH-MM-SS/last.ckpt
```

---

## è¿›é˜¶æŠ€å·§

### 1. è¶…å‚æ•°è°ƒä¼˜

**å…³é”®è¶…å‚æ•°ï¼ˆæŒ‰é‡è¦æ€§æ’åºï¼‰ï¼š**

1. **num_scalar_features** (16, 32, 64, 128, 256)
   - è¶Šå¤§è¶Šå‡†ç¡®ï¼Œä½†è¶Šæ…¢
   - ä»å°å¼€å§‹ï¼Œé€æ­¥å¢åŠ 

2. **l_max** (1, 2, 3)
   - l_max=1: å¿«é€Ÿï¼ŒåŸºç¡€ç²¾åº¦
   - l_max=2: å¹³è¡¡ç²¾åº¦å’Œé€Ÿåº¦
   - l_max=3: é«˜ç²¾åº¦ï¼Œä½†å¾ˆæ…¢

3. **num_layers** (1, 2, 3)
   - é€šå¸¸ 2-3 å±‚æœ€ä½³

4. **cutoff_radius** (4.0, 5.0, 6.0)
   - æ ¹æ®ä½ çš„ç³»ç»Ÿé€‰æ‹©
   - å¤ªå°ï¼šä¸¢å¤±é•¿ç¨‹ç›¸äº’ä½œç”¨
   - å¤ªå¤§ï¼šè®¡ç®—æˆæœ¬é«˜

**è°ƒä¼˜ç­–ç•¥ï¼š**
```bash
# åˆ›å»ºå¤šä¸ªé…ç½®æ–‡ä»¶ï¼Œé€æ­¥å¢åŠ å¤æ‚åº¦
configs/
â”œâ”€â”€ small.yaml    # num_scalar_features=32, l_max=1
â”œâ”€â”€ medium.yaml  # num_scalar_features=64, l_max=1
â””â”€â”€ large.yaml   # num_scalar_features=128, l_max=2
```

### 2. æ•°æ®å¢å¼º

- ä½¿ç”¨ä¸åŒçš„åˆå§‹ç»“æ„
- åŒ…å«ä¸åŒæ¸©åº¦å’Œå‹åŠ›ä¸‹çš„ç»“æ„
- å¹³è¡¡ä¸åŒåŒ–å­¦ç¯å¢ƒçš„æ ·æœ¬

### 3. å­¦ä¹ ç‡è°ƒåº¦

```yaml
training_module:
  optimizer:
    _target_: torch.optim.Adam
    lr: 0.001
  
  # æ·»åŠ å­¦ä¹ ç‡è°ƒåº¦å™¨
  lr_scheduler:
    _target_: torch.optim.lr_scheduler.ReduceLROnPlateau
    mode: min
    factor: 0.5
    patience: 10
```

### 4. æ—©åœç­–ç•¥

```yaml
trainer:
  callbacks:
    - _target_: lightning.pytorch.callbacks.EarlyStopping
      monitor: val0_epoch/weighted_sum
      patience: 20  # 20 ä¸ª epoch æ— æ”¹å–„åˆ™åœæ­¢
      mode: min
```

### 5. å¤š GPU è®­ç»ƒ

```yaml
trainer:
  accelerator: gpu
  devices: 2  # ä½¿ç”¨ 2 ä¸ª GPU
  strategy: ddp  # åˆ†å¸ƒå¼æ•°æ®å¹¶è¡Œ
```

### 6. æ··åˆç²¾åº¦è®­ç»ƒ

```yaml
trainer:
  precision: 16-mixed  # ä½¿ç”¨åŠç²¾åº¦ï¼ˆéœ€è¦ GPUï¼‰
```

---

## æ€»ç»“

æœ¬æ•™ç¨‹æ¶µç›–äº†ä½¿ç”¨ Allegro è®­ç»ƒåŠ¿å‡½æ•°çš„å®Œæ•´æµç¨‹ï¼š

1. âœ… ç¯å¢ƒå‡†å¤‡å’Œä¾èµ–å®‰è£…
2. âœ… æ•°æ®å‡†å¤‡å’Œæ ¼å¼è¦æ±‚
3. âœ… é…ç½®æ–‡ä»¶è¯¦è§£
4. âœ… è®­ç»ƒå¯åŠ¨å’Œç›‘æ§
5. âœ… æ¨¡å‹è¯„ä¼°å’Œä½¿ç”¨
6. âœ… å¸¸è§é—®é¢˜è§£å†³
7. âœ… è¿›é˜¶ä¼˜åŒ–æŠ€å·§

**ä¸‹ä¸€æ­¥å»ºè®®ï¼š**
- ä»ç®€å•çš„é…ç½®å¼€å§‹ï¼ˆå°æ¨¡å‹ã€å°‘é‡æ•°æ®ï¼‰
- é€æ­¥å¢åŠ æ¨¡å‹å¤æ‚åº¦å’Œæ•°æ®é‡
- æ ¹æ®éªŒè¯é›†æ€§èƒ½è°ƒæ•´è¶…å‚æ•°
- åœ¨æµ‹è¯•é›†ä¸Šæœ€ç»ˆè¯„ä¼°æ¨¡å‹

**è·å–å¸®åŠ©ï¼š**
- æŸ¥çœ‹å®˜æ–¹æ–‡æ¡£ï¼šhttps://nequip.readthedocs.io
- GitHub Issues: https://github.com/mir-group/allegro/issues
- ç¤¾åŒºè®¨è®ºï¼šhttps://github.com/mir-group/allegro/discussions

ç¥è®­ç»ƒé¡ºåˆ©ï¼ğŸ‰
